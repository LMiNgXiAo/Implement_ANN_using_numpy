{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCJ0: Cost=27531014.000, x=10494.000\n",
      "EPOCJ1: Cost=4404966.440, x=4197.600\n",
      "EPOCJ2: Cost=704798.830, x=1679.040\n",
      "EPOCJ3: Cost=112772.013, x=671.616\n",
      "EPOCJ4: Cost=18047.722, x=268.646\n",
      "EPOCJ5: Cost=2891.836, x=107.459\n",
      "EPOCJ6: Cost=466.894, x=42.983\n",
      "EPOCJ7: Cost=78.903, x=17.193\n",
      "EPOCJ8: Cost=16.824, x=6.877\n",
      "EPOCJ9: Cost=6.892, x=2.751\n",
      "EPOCJ10: Cost=5.303, x=1.100\n",
      "EPOCJ11: Cost=5.048, x=0.440\n",
      "EPOCJ12: Cost=5.008, x=0.176\n",
      "EPOCJ13: Cost=5.001, x=0.070\n",
      "EPOCJ14: Cost=5.000, x=0.028\n",
      "EPOCJ15: Cost=5.000, x=0.011\n",
      "EPOCJ16: Cost=5.000, x=0.005\n",
      "EPOCJ17: Cost=5.000, x=0.002\n",
      "EPOCJ18: Cost=5.000, x=0.001\n",
      "EPOCJ19: Cost=5.000, x=0.000\n",
      "EPOCJ20: Cost=5.000, x=0.000\n",
      "EPOCJ21: Cost=5.000, x=0.000\n",
      "EPOCJ22: Cost=5.000, x=0.000\n",
      "EPOCJ23: Cost=5.000, x=0.000\n",
      "EPOCJ24: Cost=5.000, x=0.000\n",
      "EPOCJ25: Cost=5.000, x=0.000\n",
      "EPOCJ26: Cost=5.000, x=0.000\n",
      "EPOCJ27: Cost=5.000, x=0.000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 5\n",
    "\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "old_x = float('inf')\n",
    "x = random.randint(0,10000)\n",
    "learning_rate = 0.3\n",
    "epochs = 0\n",
    "\n",
    "while abs(x-old_x) > 1.0e-7:\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    \n",
    "    old_x = x\n",
    "    x -= learning_rate * gradx\n",
    "    \n",
    "    print('EPOCJ{}: Cost={:.3f}, x={:.3f}'.format(epochs,cost,gradx))\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self,inbound_nodes=[]):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        self.outbound_nodes = []\n",
    "        \n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        #set 'self' node as inbound_noes's outbound_nodes \n",
    "        \n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients = {}\n",
    "        #keys are the inputs to this node and their\n",
    "        #values are the partials of this node with respect\n",
    "        #to that input.\n",
    "        #\\partial{node}{input_i}\n",
    "    \n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation.\n",
    "        Compute the output values based on  'inbound_nodes' and\n",
    "        store the result in self.value\n",
    "        '''\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound bodes.\n",
    "        So no need to pass anything to the Node instanttiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only the values in input node have to be passed to the forward().\n",
    "        All the others should receive the value from the previous node \n",
    "        (self.inbound_nodes)\n",
    "        \n",
    "        Example:\n",
    "        va10:self.inbound_node[0].value\n",
    "        '''\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            \n",
    "    def backward(self):\n",
    "        self.gradients={self:0}\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "        \n",
    "        # input N --> N1,N2\n",
    "        # \\partial L / \\partial N\n",
    "        # ==> \\partial L / \\partial N1 * \\partial N1 / \\partial N\n",
    "\n",
    "class CalculateNode(Node):\n",
    "    def __init__(self,f,*nodes):\n",
    "        Node.__init__(self,nodes)\n",
    "        self.func = f\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = self.func(map(lambda n: n.value, self.inbound_nodes))\n",
    "        # this function will calculate the value of the current node , when forward is executed.\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self,[nodes,weights,bias])\n",
    "    \n",
    "    def forward(self):\n",
    "        inbound_nodes = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "        \n",
    "        self.value = np.dot(inbound_nodes,weights)+bias\n",
    "    \n",
    "    def backward(self):\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            self.gradients[self.inbound_nodes[0]] = np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            self.gradients[self.inbound_nodes[1]] = np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            self.gradients[self.inbound_nodes[2]] = np.sum(grad_cost, axis=0,keepdims=False)\n",
    "            \n",
    "            #WX+B / W ==> X\n",
    "            #WX+B / X ==>W\n",
    "            \n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "    \n",
    "    def _sigmoid(self,x):\n",
    "        return 1./(1+np.exp(-1*x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.x = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x)*(1-self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1+e^-x)\n",
    "        # y'= 1 /(1+e^-x)(1-1/(1+e^-x))\n",
    "        self.gradients = {n:np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self] #Get the partial of the cost with respect to this node\n",
    "            \n",
    "        self.gradients[self.inbound_nodes[0]] = grad_cost * self.partial\n",
    "        # use * to keep all the dimension!\n",
    "            \n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y,a])\n",
    "    \n",
    "    def forward(self):\n",
    "        y = self.inbound_nodes[0].value.reshape(-1,1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1,1)\n",
    "        assert(y.shape == a.shape)\n",
    "        \n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        self.diff = y-a\n",
    "        \n",
    "        self.value = np.mean(self.diff**2)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "    \n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # excute all the forward method of sorted_nodes.\n",
    "    ## in practice, it is common to feed in mutiple data example in each forward pass rather than just 1.\n",
    "    ## because the exampls can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "       # each node execute forward, get self.value based on the topological sorted result.\n",
    "        \n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "        # return outputnode.value\n",
    "## v --> a --> c\n",
    "## b --> C\n",
    "## b -->v -->a -->c\n",
    "## v -->v --> a -->c\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm\n",
    "    'feed_dict': A dictionary where the key is a 'Input' node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "    \n",
    "    G={}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n  not in G:\n",
    "            G[n] = {'in': set(), 'out':set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in':set(),'out':set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "            \n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "        \n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n.value as\n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its inbounds\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    for t in trainables:\n",
    "        t.value -= learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 21.382\n",
      "Epoch: 1, Loss: 33.644\n",
      "Epoch: 1, Loss: 44.811\n",
      "Epoch: 1, Loss: 52.369\n",
      "Epoch: 1, Loss: 61.135\n",
      "Epoch: 1, Loss: 68.180\n",
      "Epoch: 1, Loss: 76.966\n",
      "Epoch: 1, Loss: 83.839\n",
      "Epoch: 1, Loss: 86.731\n",
      "Epoch: 1, Loss: 95.028\n",
      "Epoch: 1, Loss: 98.956\n",
      "Epoch: 1, Loss: 104.270\n",
      "Epoch: 1, Loss: 108.802\n",
      "Epoch: 1, Loss: 112.999\n",
      "Epoch: 1, Loss: 117.951\n",
      "Epoch: 1, Loss: 118.515\n",
      "Epoch: 1, Loss: 120.010\n",
      "Epoch: 1, Loss: 120.734\n",
      "Epoch: 1, Loss: 121.157\n",
      "Epoch: 1, Loss: 122.114\n",
      "Epoch: 1, Loss: 124.255\n",
      "Epoch: 1, Loss: 125.891\n",
      "Epoch: 1, Loss: 127.570\n",
      "Epoch: 1, Loss: 127.994\n",
      "Epoch: 1, Loss: 128.508\n",
      "Epoch: 1, Loss: 130.964\n",
      "Epoch: 1, Loss: 131.763\n",
      "Epoch: 1, Loss: 132.223\n",
      "Epoch: 1, Loss: 134.215\n",
      "Epoch: 1, Loss: 135.109\n",
      "Epoch: 1, Loss: 135.946\n",
      "Epoch: 101, Loss: 0.277\n",
      "Epoch: 101, Loss: 0.401\n",
      "Epoch: 101, Loss: 0.807\n",
      "Epoch: 101, Loss: 1.464\n",
      "Epoch: 101, Loss: 1.556\n",
      "Epoch: 101, Loss: 1.986\n",
      "Epoch: 101, Loss: 2.103\n",
      "Epoch: 101, Loss: 2.380\n",
      "Epoch: 101, Loss: 3.003\n",
      "Epoch: 101, Loss: 3.201\n",
      "Epoch: 101, Loss: 3.329\n",
      "Epoch: 101, Loss: 3.433\n",
      "Epoch: 101, Loss: 3.666\n",
      "Epoch: 101, Loss: 4.357\n",
      "Epoch: 101, Loss: 4.779\n",
      "Epoch: 101, Loss: 5.117\n",
      "Epoch: 101, Loss: 5.205\n",
      "Epoch: 101, Loss: 5.677\n",
      "Epoch: 101, Loss: 6.216\n",
      "Epoch: 101, Loss: 6.329\n",
      "Epoch: 101, Loss: 6.538\n",
      "Epoch: 101, Loss: 6.718\n",
      "Epoch: 101, Loss: 6.864\n",
      "Epoch: 101, Loss: 6.991\n",
      "Epoch: 101, Loss: 7.178\n",
      "Epoch: 101, Loss: 7.533\n",
      "Epoch: 101, Loss: 7.740\n",
      "Epoch: 101, Loss: 7.928\n",
      "Epoch: 101, Loss: 8.177\n",
      "Epoch: 101, Loss: 8.506\n",
      "Epoch: 101, Loss: 8.760\n",
      "Epoch: 201, Loss: 0.325\n",
      "Epoch: 201, Loss: 0.777\n",
      "Epoch: 201, Loss: 0.982\n",
      "Epoch: 201, Loss: 1.184\n",
      "Epoch: 201, Loss: 1.271\n",
      "Epoch: 201, Loss: 1.546\n",
      "Epoch: 201, Loss: 1.666\n",
      "Epoch: 201, Loss: 1.782\n",
      "Epoch: 201, Loss: 1.892\n",
      "Epoch: 201, Loss: 2.058\n",
      "Epoch: 201, Loss: 2.150\n",
      "Epoch: 201, Loss: 2.228\n",
      "Epoch: 201, Loss: 2.374\n",
      "Epoch: 201, Loss: 2.487\n",
      "Epoch: 201, Loss: 2.710\n",
      "Epoch: 201, Loss: 3.296\n",
      "Epoch: 201, Loss: 3.497\n",
      "Epoch: 201, Loss: 3.630\n",
      "Epoch: 201, Loss: 4.013\n",
      "Epoch: 201, Loss: 4.232\n",
      "Epoch: 201, Loss: 4.655\n",
      "Epoch: 201, Loss: 5.093\n",
      "Epoch: 201, Loss: 5.465\n",
      "Epoch: 201, Loss: 5.730\n",
      "Epoch: 201, Loss: 5.976\n",
      "Epoch: 201, Loss: 6.355\n",
      "Epoch: 201, Loss: 6.634\n",
      "Epoch: 201, Loss: 6.770\n",
      "Epoch: 201, Loss: 6.862\n",
      "Epoch: 201, Loss: 7.163\n",
      "Epoch: 201, Loss: 7.612\n",
      "Epoch: 301, Loss: 0.214\n",
      "Epoch: 301, Loss: 0.336\n",
      "Epoch: 301, Loss: 0.446\n",
      "Epoch: 301, Loss: 0.596\n",
      "Epoch: 301, Loss: 0.774\n",
      "Epoch: 301, Loss: 0.887\n",
      "Epoch: 301, Loss: 1.034\n",
      "Epoch: 301, Loss: 1.125\n",
      "Epoch: 301, Loss: 1.362\n",
      "Epoch: 301, Loss: 1.511\n",
      "Epoch: 301, Loss: 1.620\n",
      "Epoch: 301, Loss: 1.868\n",
      "Epoch: 301, Loss: 2.067\n",
      "Epoch: 301, Loss: 2.221\n",
      "Epoch: 301, Loss: 2.521\n",
      "Epoch: 301, Loss: 2.643\n",
      "Epoch: 301, Loss: 2.838\n",
      "Epoch: 301, Loss: 2.946\n",
      "Epoch: 301, Loss: 3.031\n",
      "Epoch: 301, Loss: 3.258\n",
      "Epoch: 301, Loss: 3.502\n",
      "Epoch: 301, Loss: 3.674\n",
      "Epoch: 301, Loss: 3.729\n",
      "Epoch: 301, Loss: 3.905\n",
      "Epoch: 301, Loss: 4.048\n",
      "Epoch: 301, Loss: 4.188\n",
      "Epoch: 301, Loss: 4.281\n",
      "Epoch: 301, Loss: 4.418\n",
      "Epoch: 301, Loss: 4.532\n",
      "Epoch: 301, Loss: 4.708\n",
      "Epoch: 301, Loss: 4.908\n",
      "Epoch: 401, Loss: 0.113\n",
      "Epoch: 401, Loss: 0.273\n",
      "Epoch: 401, Loss: 0.727\n",
      "Epoch: 401, Loss: 0.924\n",
      "Epoch: 401, Loss: 1.039\n",
      "Epoch: 401, Loss: 1.283\n",
      "Epoch: 401, Loss: 1.445\n",
      "Epoch: 401, Loss: 1.570\n",
      "Epoch: 401, Loss: 1.740\n",
      "Epoch: 401, Loss: 1.849\n",
      "Epoch: 401, Loss: 1.983\n",
      "Epoch: 401, Loss: 2.080\n",
      "Epoch: 401, Loss: 2.186\n",
      "Epoch: 401, Loss: 2.349\n",
      "Epoch: 401, Loss: 2.585\n",
      "Epoch: 401, Loss: 2.723\n",
      "Epoch: 401, Loss: 2.858\n",
      "Epoch: 401, Loss: 3.038\n",
      "Epoch: 401, Loss: 3.364\n",
      "Epoch: 401, Loss: 3.415\n",
      "Epoch: 401, Loss: 3.535\n",
      "Epoch: 401, Loss: 3.658\n",
      "Epoch: 401, Loss: 3.767\n",
      "Epoch: 401, Loss: 4.021\n",
      "Epoch: 401, Loss: 4.120\n",
      "Epoch: 401, Loss: 4.175\n",
      "Epoch: 401, Loss: 4.380\n",
      "Epoch: 401, Loss: 4.613\n",
      "Epoch: 401, Loss: 4.744\n",
      "Epoch: 401, Loss: 4.871\n",
      "Epoch: 401, Loss: 4.949\n",
      "Epoch: 501, Loss: 0.109\n",
      "Epoch: 501, Loss: 0.230\n",
      "Epoch: 501, Loss: 0.443\n",
      "Epoch: 501, Loss: 0.594\n",
      "Epoch: 501, Loss: 0.706\n",
      "Epoch: 501, Loss: 0.960\n",
      "Epoch: 501, Loss: 1.118\n",
      "Epoch: 501, Loss: 1.224\n",
      "Epoch: 501, Loss: 1.345\n",
      "Epoch: 501, Loss: 1.555\n",
      "Epoch: 501, Loss: 1.697\n",
      "Epoch: 501, Loss: 1.921\n",
      "Epoch: 501, Loss: 2.228\n",
      "Epoch: 501, Loss: 2.470\n",
      "Epoch: 501, Loss: 2.634\n",
      "Epoch: 501, Loss: 2.995\n",
      "Epoch: 501, Loss: 3.250\n",
      "Epoch: 501, Loss: 3.380\n",
      "Epoch: 501, Loss: 3.591\n",
      "Epoch: 501, Loss: 3.969\n",
      "Epoch: 501, Loss: 4.048\n",
      "Epoch: 501, Loss: 4.121\n",
      "Epoch: 501, Loss: 4.227\n",
      "Epoch: 501, Loss: 4.486\n",
      "Epoch: 501, Loss: 4.762\n",
      "Epoch: 501, Loss: 4.886\n",
      "Epoch: 501, Loss: 5.190\n",
      "Epoch: 501, Loss: 5.275\n",
      "Epoch: 501, Loss: 5.437\n",
      "Epoch: 501, Loss: 5.798\n",
      "Epoch: 501, Loss: 6.047\n",
      "Epoch: 601, Loss: 0.086\n",
      "Epoch: 601, Loss: 0.190\n",
      "Epoch: 601, Loss: 0.266\n",
      "Epoch: 601, Loss: 0.384\n",
      "Epoch: 601, Loss: 0.510\n",
      "Epoch: 601, Loss: 0.969\n",
      "Epoch: 601, Loss: 1.070\n",
      "Epoch: 601, Loss: 1.207\n",
      "Epoch: 601, Loss: 1.285\n",
      "Epoch: 601, Loss: 1.409\n",
      "Epoch: 601, Loss: 1.507\n",
      "Epoch: 601, Loss: 1.553\n",
      "Epoch: 601, Loss: 1.651\n",
      "Epoch: 601, Loss: 1.875\n",
      "Epoch: 601, Loss: 2.178\n",
      "Epoch: 601, Loss: 2.287\n",
      "Epoch: 601, Loss: 2.367\n",
      "Epoch: 601, Loss: 2.506\n",
      "Epoch: 601, Loss: 2.592\n",
      "Epoch: 601, Loss: 2.699\n",
      "Epoch: 601, Loss: 2.870\n",
      "Epoch: 601, Loss: 3.175\n",
      "Epoch: 601, Loss: 3.353\n",
      "Epoch: 601, Loss: 3.568\n",
      "Epoch: 601, Loss: 3.834\n",
      "Epoch: 601, Loss: 4.020\n",
      "Epoch: 601, Loss: 4.187\n",
      "Epoch: 601, Loss: 4.355\n",
      "Epoch: 601, Loss: 4.549\n",
      "Epoch: 601, Loss: 4.614\n",
      "Epoch: 601, Loss: 4.692\n",
      "Epoch: 701, Loss: 0.339\n",
      "Epoch: 701, Loss: 0.491\n",
      "Epoch: 701, Loss: 0.575\n",
      "Epoch: 701, Loss: 0.652\n",
      "Epoch: 701, Loss: 0.920\n",
      "Epoch: 701, Loss: 1.047\n",
      "Epoch: 701, Loss: 1.142\n",
      "Epoch: 701, Loss: 1.227\n",
      "Epoch: 701, Loss: 1.303\n",
      "Epoch: 701, Loss: 1.394\n",
      "Epoch: 701, Loss: 1.504\n",
      "Epoch: 701, Loss: 1.623\n",
      "Epoch: 701, Loss: 1.681\n",
      "Epoch: 701, Loss: 1.876\n",
      "Epoch: 701, Loss: 1.937\n",
      "Epoch: 701, Loss: 2.150\n",
      "Epoch: 701, Loss: 2.329\n",
      "Epoch: 701, Loss: 2.582\n",
      "Epoch: 701, Loss: 2.710\n",
      "Epoch: 701, Loss: 2.894\n",
      "Epoch: 701, Loss: 3.136\n",
      "Epoch: 701, Loss: 3.220\n",
      "Epoch: 701, Loss: 3.361\n",
      "Epoch: 701, Loss: 3.453\n",
      "Epoch: 701, Loss: 3.790\n",
      "Epoch: 701, Loss: 3.960\n",
      "Epoch: 701, Loss: 4.253\n",
      "Epoch: 701, Loss: 4.345\n",
      "Epoch: 701, Loss: 4.478\n",
      "Epoch: 701, Loss: 4.556\n",
      "Epoch: 701, Loss: 4.677\n",
      "Epoch: 801, Loss: 0.157\n",
      "Epoch: 801, Loss: 0.260\n",
      "Epoch: 801, Loss: 0.459\n",
      "Epoch: 801, Loss: 0.633\n",
      "Epoch: 801, Loss: 0.710\n",
      "Epoch: 801, Loss: 0.750\n",
      "Epoch: 801, Loss: 0.812\n",
      "Epoch: 801, Loss: 0.861\n",
      "Epoch: 801, Loss: 0.958\n",
      "Epoch: 801, Loss: 1.064\n",
      "Epoch: 801, Loss: 1.509\n",
      "Epoch: 801, Loss: 1.627\n",
      "Epoch: 801, Loss: 1.777\n",
      "Epoch: 801, Loss: 2.027\n",
      "Epoch: 801, Loss: 2.137\n",
      "Epoch: 801, Loss: 2.211\n",
      "Epoch: 801, Loss: 2.289\n",
      "Epoch: 801, Loss: 2.420\n",
      "Epoch: 801, Loss: 2.485\n",
      "Epoch: 801, Loss: 2.593\n",
      "Epoch: 801, Loss: 2.712\n",
      "Epoch: 801, Loss: 2.783\n",
      "Epoch: 801, Loss: 2.870\n",
      "Epoch: 801, Loss: 2.989\n",
      "Epoch: 801, Loss: 3.326\n",
      "Epoch: 801, Loss: 3.466\n",
      "Epoch: 801, Loss: 3.655\n",
      "Epoch: 801, Loss: 3.764\n",
      "Epoch: 801, Loss: 3.827\n",
      "Epoch: 801, Loss: 3.915\n",
      "Epoch: 801, Loss: 4.051\n",
      "Epoch: 901, Loss: 0.127\n",
      "Epoch: 901, Loss: 0.301\n",
      "Epoch: 901, Loss: 0.475\n",
      "Epoch: 901, Loss: 0.706\n",
      "Epoch: 901, Loss: 0.938\n",
      "Epoch: 901, Loss: 1.027\n",
      "Epoch: 901, Loss: 1.111\n",
      "Epoch: 901, Loss: 1.222\n",
      "Epoch: 901, Loss: 1.566\n",
      "Epoch: 901, Loss: 1.769\n",
      "Epoch: 901, Loss: 1.809\n",
      "Epoch: 901, Loss: 1.923\n",
      "Epoch: 901, Loss: 2.286\n",
      "Epoch: 901, Loss: 2.366\n",
      "Epoch: 901, Loss: 2.501\n",
      "Epoch: 901, Loss: 2.570\n",
      "Epoch: 901, Loss: 2.649\n",
      "Epoch: 901, Loss: 2.693\n",
      "Epoch: 901, Loss: 2.807\n",
      "Epoch: 901, Loss: 2.915\n",
      "Epoch: 901, Loss: 3.076\n",
      "Epoch: 901, Loss: 3.216\n",
      "Epoch: 901, Loss: 3.294\n",
      "Epoch: 901, Loss: 3.391\n",
      "Epoch: 901, Loss: 3.558\n",
      "Epoch: 901, Loss: 3.651\n",
      "Epoch: 901, Loss: 3.762\n",
      "Epoch: 901, Loss: 3.952\n",
      "Epoch: 901, Loss: 4.022\n",
      "Epoch: 901, Loss: 4.203\n",
      "Epoch: 901, Loss: 4.338\n",
      "Epoch: 1001, Loss: 0.071\n",
      "Epoch: 1001, Loss: 0.239\n",
      "Epoch: 1001, Loss: 0.512\n",
      "Epoch: 1001, Loss: 0.568\n",
      "Epoch: 1001, Loss: 0.748\n",
      "Epoch: 1001, Loss: 0.894\n",
      "Epoch: 1001, Loss: 1.275\n",
      "Epoch: 1001, Loss: 1.689\n",
      "Epoch: 1001, Loss: 1.807\n",
      "Epoch: 1001, Loss: 1.905\n",
      "Epoch: 1001, Loss: 1.990\n",
      "Epoch: 1001, Loss: 2.130\n",
      "Epoch: 1001, Loss: 2.211\n",
      "Epoch: 1001, Loss: 2.287\n",
      "Epoch: 1001, Loss: 2.472\n",
      "Epoch: 1001, Loss: 2.560\n",
      "Epoch: 1001, Loss: 2.710\n",
      "Epoch: 1001, Loss: 2.860\n",
      "Epoch: 1001, Loss: 3.017\n",
      "Epoch: 1001, Loss: 3.156\n",
      "Epoch: 1001, Loss: 3.208\n",
      "Epoch: 1001, Loss: 3.283\n",
      "Epoch: 1001, Loss: 3.347\n",
      "Epoch: 1001, Loss: 3.475\n",
      "Epoch: 1001, Loss: 3.665\n",
      "Epoch: 1001, Loss: 3.740\n",
      "Epoch: 1001, Loss: 3.977\n",
      "Epoch: 1001, Loss: 4.048\n",
      "Epoch: 1001, Loss: 4.185\n",
      "Epoch: 1001, Loss: 4.391\n",
      "Epoch: 1001, Loss: 4.540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1101, Loss: 0.057\n",
      "Epoch: 1101, Loss: 0.153\n",
      "Epoch: 1101, Loss: 0.261\n",
      "Epoch: 1101, Loss: 0.415\n",
      "Epoch: 1101, Loss: 0.494\n",
      "Epoch: 1101, Loss: 0.665\n",
      "Epoch: 1101, Loss: 0.940\n",
      "Epoch: 1101, Loss: 1.074\n",
      "Epoch: 1101, Loss: 1.176\n",
      "Epoch: 1101, Loss: 1.428\n",
      "Epoch: 1101, Loss: 1.586\n",
      "Epoch: 1101, Loss: 1.744\n",
      "Epoch: 1101, Loss: 1.797\n",
      "Epoch: 1101, Loss: 1.940\n",
      "Epoch: 1101, Loss: 1.987\n",
      "Epoch: 1101, Loss: 2.261\n",
      "Epoch: 1101, Loss: 2.433\n",
      "Epoch: 1101, Loss: 2.566\n",
      "Epoch: 1101, Loss: 2.702\n",
      "Epoch: 1101, Loss: 2.810\n",
      "Epoch: 1101, Loss: 2.847\n",
      "Epoch: 1101, Loss: 2.962\n",
      "Epoch: 1101, Loss: 3.072\n",
      "Epoch: 1101, Loss: 3.233\n",
      "Epoch: 1101, Loss: 3.438\n",
      "Epoch: 1101, Loss: 3.511\n",
      "Epoch: 1101, Loss: 3.564\n",
      "Epoch: 1101, Loss: 3.695\n",
      "Epoch: 1101, Loss: 3.845\n",
      "Epoch: 1101, Loss: 4.147\n",
      "Epoch: 1101, Loss: 4.208\n",
      "Epoch: 1201, Loss: 0.106\n",
      "Epoch: 1201, Loss: 0.342\n",
      "Epoch: 1201, Loss: 0.389\n",
      "Epoch: 1201, Loss: 0.581\n",
      "Epoch: 1201, Loss: 0.674\n",
      "Epoch: 1201, Loss: 0.785\n",
      "Epoch: 1201, Loss: 0.827\n",
      "Epoch: 1201, Loss: 0.937\n",
      "Epoch: 1201, Loss: 1.145\n",
      "Epoch: 1201, Loss: 1.228\n",
      "Epoch: 1201, Loss: 1.344\n",
      "Epoch: 1201, Loss: 1.417\n",
      "Epoch: 1201, Loss: 1.492\n",
      "Epoch: 1201, Loss: 1.565\n",
      "Epoch: 1201, Loss: 1.718\n",
      "Epoch: 1201, Loss: 1.761\n",
      "Epoch: 1201, Loss: 1.830\n",
      "Epoch: 1201, Loss: 1.971\n",
      "Epoch: 1201, Loss: 2.041\n",
      "Epoch: 1201, Loss: 2.217\n",
      "Epoch: 1201, Loss: 2.409\n",
      "Epoch: 1201, Loss: 2.509\n",
      "Epoch: 1201, Loss: 2.629\n",
      "Epoch: 1201, Loss: 2.700\n",
      "Epoch: 1201, Loss: 2.883\n",
      "Epoch: 1201, Loss: 3.104\n",
      "Epoch: 1201, Loss: 3.173\n",
      "Epoch: 1201, Loss: 3.465\n",
      "Epoch: 1201, Loss: 3.618\n",
      "Epoch: 1201, Loss: 3.761\n",
      "Epoch: 1201, Loss: 3.897\n",
      "Epoch: 1301, Loss: 0.197\n",
      "Epoch: 1301, Loss: 0.337\n",
      "Epoch: 1301, Loss: 0.469\n",
      "Epoch: 1301, Loss: 0.662\n",
      "Epoch: 1301, Loss: 0.740\n",
      "Epoch: 1301, Loss: 0.812\n",
      "Epoch: 1301, Loss: 0.883\n",
      "Epoch: 1301, Loss: 0.945\n",
      "Epoch: 1301, Loss: 1.137\n",
      "Epoch: 1301, Loss: 1.256\n",
      "Epoch: 1301, Loss: 1.365\n",
      "Epoch: 1301, Loss: 1.459\n",
      "Epoch: 1301, Loss: 1.587\n",
      "Epoch: 1301, Loss: 1.805\n",
      "Epoch: 1301, Loss: 1.974\n",
      "Epoch: 1301, Loss: 2.219\n",
      "Epoch: 1301, Loss: 2.321\n",
      "Epoch: 1301, Loss: 2.373\n",
      "Epoch: 1301, Loss: 2.505\n",
      "Epoch: 1301, Loss: 2.544\n",
      "Epoch: 1301, Loss: 2.608\n",
      "Epoch: 1301, Loss: 2.661\n",
      "Epoch: 1301, Loss: 2.871\n",
      "Epoch: 1301, Loss: 2.985\n",
      "Epoch: 1301, Loss: 3.244\n",
      "Epoch: 1301, Loss: 3.373\n",
      "Epoch: 1301, Loss: 3.508\n",
      "Epoch: 1301, Loss: 3.830\n",
      "Epoch: 1301, Loss: 3.943\n",
      "Epoch: 1301, Loss: 4.052\n",
      "Epoch: 1301, Loss: 4.182\n",
      "Epoch: 1401, Loss: 0.207\n",
      "Epoch: 1401, Loss: 0.393\n",
      "Epoch: 1401, Loss: 0.462\n",
      "Epoch: 1401, Loss: 0.673\n",
      "Epoch: 1401, Loss: 0.787\n",
      "Epoch: 1401, Loss: 0.879\n",
      "Epoch: 1401, Loss: 0.998\n",
      "Epoch: 1401, Loss: 1.163\n",
      "Epoch: 1401, Loss: 1.298\n",
      "Epoch: 1401, Loss: 1.385\n",
      "Epoch: 1401, Loss: 1.481\n",
      "Epoch: 1401, Loss: 1.617\n",
      "Epoch: 1401, Loss: 1.887\n",
      "Epoch: 1401, Loss: 1.963\n",
      "Epoch: 1401, Loss: 2.169\n",
      "Epoch: 1401, Loss: 2.250\n",
      "Epoch: 1401, Loss: 2.399\n",
      "Epoch: 1401, Loss: 2.506\n",
      "Epoch: 1401, Loss: 2.677\n",
      "Epoch: 1401, Loss: 2.835\n",
      "Epoch: 1401, Loss: 2.907\n",
      "Epoch: 1401, Loss: 3.020\n",
      "Epoch: 1401, Loss: 3.130\n",
      "Epoch: 1401, Loss: 3.347\n",
      "Epoch: 1401, Loss: 3.513\n",
      "Epoch: 1401, Loss: 3.630\n",
      "Epoch: 1401, Loss: 3.776\n",
      "Epoch: 1401, Loss: 3.878\n",
      "Epoch: 1401, Loss: 3.942\n",
      "Epoch: 1401, Loss: 4.020\n",
      "Epoch: 1401, Loss: 4.226\n",
      "Epoch: 1501, Loss: 0.176\n",
      "Epoch: 1501, Loss: 0.300\n",
      "Epoch: 1501, Loss: 0.558\n",
      "Epoch: 1501, Loss: 0.774\n",
      "Epoch: 1501, Loss: 0.955\n",
      "Epoch: 1501, Loss: 1.106\n",
      "Epoch: 1501, Loss: 1.200\n",
      "Epoch: 1501, Loss: 1.287\n",
      "Epoch: 1501, Loss: 1.416\n",
      "Epoch: 1501, Loss: 1.494\n",
      "Epoch: 1501, Loss: 1.627\n",
      "Epoch: 1501, Loss: 1.689\n",
      "Epoch: 1501, Loss: 1.873\n",
      "Epoch: 1501, Loss: 2.062\n",
      "Epoch: 1501, Loss: 2.164\n",
      "Epoch: 1501, Loss: 2.244\n",
      "Epoch: 1501, Loss: 2.325\n",
      "Epoch: 1501, Loss: 2.395\n",
      "Epoch: 1501, Loss: 2.535\n",
      "Epoch: 1501, Loss: 2.706\n",
      "Epoch: 1501, Loss: 2.792\n",
      "Epoch: 1501, Loss: 2.865\n",
      "Epoch: 1501, Loss: 2.964\n",
      "Epoch: 1501, Loss: 3.057\n",
      "Epoch: 1501, Loss: 3.135\n",
      "Epoch: 1501, Loss: 3.345\n",
      "Epoch: 1501, Loss: 3.428\n",
      "Epoch: 1501, Loss: 3.580\n",
      "Epoch: 1501, Loss: 3.740\n",
      "Epoch: 1501, Loss: 4.030\n",
      "Epoch: 1501, Loss: 4.225\n",
      "Epoch: 1601, Loss: 0.061\n",
      "Epoch: 1601, Loss: 0.180\n",
      "Epoch: 1601, Loss: 0.436\n",
      "Epoch: 1601, Loss: 0.520\n",
      "Epoch: 1601, Loss: 0.555\n",
      "Epoch: 1601, Loss: 0.616\n",
      "Epoch: 1601, Loss: 0.670\n",
      "Epoch: 1601, Loss: 0.778\n",
      "Epoch: 1601, Loss: 0.867\n",
      "Epoch: 1601, Loss: 0.973\n",
      "Epoch: 1601, Loss: 1.146\n",
      "Epoch: 1601, Loss: 1.314\n",
      "Epoch: 1601, Loss: 1.519\n",
      "Epoch: 1601, Loss: 1.747\n",
      "Epoch: 1601, Loss: 1.841\n",
      "Epoch: 1601, Loss: 2.010\n",
      "Epoch: 1601, Loss: 2.143\n",
      "Epoch: 1601, Loss: 2.462\n",
      "Epoch: 1601, Loss: 2.634\n",
      "Epoch: 1601, Loss: 2.749\n",
      "Epoch: 1601, Loss: 2.942\n",
      "Epoch: 1601, Loss: 3.002\n",
      "Epoch: 1601, Loss: 3.140\n",
      "Epoch: 1601, Loss: 3.222\n",
      "Epoch: 1601, Loss: 3.329\n",
      "Epoch: 1601, Loss: 3.593\n",
      "Epoch: 1601, Loss: 3.728\n",
      "Epoch: 1601, Loss: 3.831\n",
      "Epoch: 1601, Loss: 3.889\n",
      "Epoch: 1601, Loss: 3.947\n",
      "Epoch: 1601, Loss: 4.053\n",
      "Epoch: 1701, Loss: 0.100\n",
      "Epoch: 1701, Loss: 0.160\n",
      "Epoch: 1701, Loss: 0.236\n",
      "Epoch: 1701, Loss: 0.381\n",
      "Epoch: 1701, Loss: 0.508\n",
      "Epoch: 1701, Loss: 0.572\n",
      "Epoch: 1701, Loss: 0.838\n",
      "Epoch: 1701, Loss: 0.968\n",
      "Epoch: 1701, Loss: 1.065\n",
      "Epoch: 1701, Loss: 1.118\n",
      "Epoch: 1701, Loss: 1.265\n",
      "Epoch: 1701, Loss: 1.328\n",
      "Epoch: 1701, Loss: 1.485\n",
      "Epoch: 1701, Loss: 1.693\n",
      "Epoch: 1701, Loss: 1.817\n",
      "Epoch: 1701, Loss: 2.050\n",
      "Epoch: 1701, Loss: 2.104\n",
      "Epoch: 1701, Loss: 2.216\n",
      "Epoch: 1701, Loss: 2.365\n",
      "Epoch: 1701, Loss: 2.507\n",
      "Epoch: 1701, Loss: 2.602\n",
      "Epoch: 1701, Loss: 2.735\n",
      "Epoch: 1701, Loss: 2.797\n",
      "Epoch: 1701, Loss: 2.920\n",
      "Epoch: 1701, Loss: 3.131\n",
      "Epoch: 1701, Loss: 3.240\n",
      "Epoch: 1701, Loss: 3.391\n",
      "Epoch: 1701, Loss: 3.586\n",
      "Epoch: 1701, Loss: 3.670\n",
      "Epoch: 1701, Loss: 3.769\n",
      "Epoch: 1701, Loss: 3.908\n",
      "Epoch: 1801, Loss: 0.131\n",
      "Epoch: 1801, Loss: 0.202\n",
      "Epoch: 1801, Loss: 0.348\n",
      "Epoch: 1801, Loss: 0.561\n",
      "Epoch: 1801, Loss: 0.648\n",
      "Epoch: 1801, Loss: 0.827\n",
      "Epoch: 1801, Loss: 0.900\n",
      "Epoch: 1801, Loss: 1.034\n",
      "Epoch: 1801, Loss: 1.077\n",
      "Epoch: 1801, Loss: 1.116\n",
      "Epoch: 1801, Loss: 1.180\n",
      "Epoch: 1801, Loss: 1.289\n",
      "Epoch: 1801, Loss: 1.431\n",
      "Epoch: 1801, Loss: 1.610\n",
      "Epoch: 1801, Loss: 1.761\n",
      "Epoch: 1801, Loss: 1.859\n",
      "Epoch: 1801, Loss: 1.964\n",
      "Epoch: 1801, Loss: 2.108\n",
      "Epoch: 1801, Loss: 2.247\n",
      "Epoch: 1801, Loss: 2.352\n",
      "Epoch: 1801, Loss: 2.408\n",
      "Epoch: 1801, Loss: 2.577\n",
      "Epoch: 1801, Loss: 2.669\n",
      "Epoch: 1801, Loss: 2.773\n",
      "Epoch: 1801, Loss: 2.887\n",
      "Epoch: 1801, Loss: 2.990\n",
      "Epoch: 1801, Loss: 3.131\n",
      "Epoch: 1801, Loss: 3.251\n",
      "Epoch: 1801, Loss: 3.431\n",
      "Epoch: 1801, Loss: 3.590\n",
      "Epoch: 1801, Loss: 3.673\n",
      "Epoch: 1901, Loss: 0.142\n",
      "Epoch: 1901, Loss: 0.202\n",
      "Epoch: 1901, Loss: 0.299\n",
      "Epoch: 1901, Loss: 0.364\n",
      "Epoch: 1901, Loss: 0.467\n",
      "Epoch: 1901, Loss: 0.678\n",
      "Epoch: 1901, Loss: 0.820\n",
      "Epoch: 1901, Loss: 0.921\n",
      "Epoch: 1901, Loss: 1.084\n",
      "Epoch: 1901, Loss: 1.209\n",
      "Epoch: 1901, Loss: 1.498\n",
      "Epoch: 1901, Loss: 1.578\n",
      "Epoch: 1901, Loss: 1.659\n",
      "Epoch: 1901, Loss: 1.727\n",
      "Epoch: 1901, Loss: 1.897\n",
      "Epoch: 1901, Loss: 1.976\n",
      "Epoch: 1901, Loss: 2.043\n",
      "Epoch: 1901, Loss: 2.173\n",
      "Epoch: 1901, Loss: 2.270\n",
      "Epoch: 1901, Loss: 2.368\n",
      "Epoch: 1901, Loss: 2.477\n",
      "Epoch: 1901, Loss: 2.563\n",
      "Epoch: 1901, Loss: 2.650\n",
      "Epoch: 1901, Loss: 2.847\n",
      "Epoch: 1901, Loss: 3.008\n",
      "Epoch: 1901, Loss: 3.072\n",
      "Epoch: 1901, Loss: 3.149\n",
      "Epoch: 1901, Loss: 3.232\n",
      "Epoch: 1901, Loss: 3.466\n",
      "Epoch: 1901, Loss: 3.630\n",
      "Epoch: 1901, Loss: 3.809\n",
      "Epoch: 2001, Loss: 0.115\n",
      "Epoch: 2001, Loss: 0.198\n",
      "Epoch: 2001, Loss: 0.377\n",
      "Epoch: 2001, Loss: 0.608\n",
      "Epoch: 2001, Loss: 0.653\n",
      "Epoch: 2001, Loss: 0.807\n",
      "Epoch: 2001, Loss: 0.930\n",
      "Epoch: 2001, Loss: 1.035\n",
      "Epoch: 2001, Loss: 1.159\n",
      "Epoch: 2001, Loss: 1.272\n",
      "Epoch: 2001, Loss: 1.432\n",
      "Epoch: 2001, Loss: 1.664\n",
      "Epoch: 2001, Loss: 1.762\n",
      "Epoch: 2001, Loss: 1.852\n",
      "Epoch: 2001, Loss: 2.208\n",
      "Epoch: 2001, Loss: 2.376\n",
      "Epoch: 2001, Loss: 2.486\n",
      "Epoch: 2001, Loss: 2.612\n",
      "Epoch: 2001, Loss: 2.729\n",
      "Epoch: 2001, Loss: 2.910\n",
      "Epoch: 2001, Loss: 3.037\n",
      "Epoch: 2001, Loss: 3.145\n",
      "Epoch: 2001, Loss: 3.239\n",
      "Epoch: 2001, Loss: 3.327\n",
      "Epoch: 2001, Loss: 3.544\n",
      "Epoch: 2001, Loss: 3.708\n",
      "Epoch: 2001, Loss: 3.818\n",
      "Epoch: 2001, Loss: 3.909\n",
      "Epoch: 2001, Loss: 4.020\n",
      "Epoch: 2001, Loss: 4.060\n",
      "Epoch: 2001, Loss: 4.117\n",
      "Epoch: 2101, Loss: 0.169\n",
      "Epoch: 2101, Loss: 0.233\n",
      "Epoch: 2101, Loss: 0.337\n",
      "Epoch: 2101, Loss: 0.462\n",
      "Epoch: 2101, Loss: 0.597\n",
      "Epoch: 2101, Loss: 0.703\n",
      "Epoch: 2101, Loss: 0.838\n",
      "Epoch: 2101, Loss: 0.954\n",
      "Epoch: 2101, Loss: 1.103\n",
      "Epoch: 2101, Loss: 1.256\n",
      "Epoch: 2101, Loss: 1.378\n",
      "Epoch: 2101, Loss: 1.548\n",
      "Epoch: 2101, Loss: 1.913\n",
      "Epoch: 2101, Loss: 2.025\n",
      "Epoch: 2101, Loss: 2.147\n",
      "Epoch: 2101, Loss: 2.289\n",
      "Epoch: 2101, Loss: 2.367\n",
      "Epoch: 2101, Loss: 2.580\n",
      "Epoch: 2101, Loss: 2.711\n",
      "Epoch: 2101, Loss: 2.896\n",
      "Epoch: 2101, Loss: 2.998\n",
      "Epoch: 2101, Loss: 3.120\n",
      "Epoch: 2101, Loss: 3.374\n",
      "Epoch: 2101, Loss: 3.576\n",
      "Epoch: 2101, Loss: 3.666\n",
      "Epoch: 2101, Loss: 3.836\n",
      "Epoch: 2101, Loss: 3.931\n",
      "Epoch: 2101, Loss: 4.093\n",
      "Epoch: 2101, Loss: 4.174\n",
      "Epoch: 2101, Loss: 4.513\n",
      "Epoch: 2101, Loss: 4.650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2201, Loss: 0.159\n",
      "Epoch: 2201, Loss: 0.247\n",
      "Epoch: 2201, Loss: 0.454\n",
      "Epoch: 2201, Loss: 0.490\n",
      "Epoch: 2201, Loss: 0.594\n",
      "Epoch: 2201, Loss: 0.695\n",
      "Epoch: 2201, Loss: 0.790\n",
      "Epoch: 2201, Loss: 0.903\n",
      "Epoch: 2201, Loss: 1.006\n",
      "Epoch: 2201, Loss: 1.229\n",
      "Epoch: 2201, Loss: 1.316\n",
      "Epoch: 2201, Loss: 1.402\n",
      "Epoch: 2201, Loss: 1.472\n",
      "Epoch: 2201, Loss: 1.564\n",
      "Epoch: 2201, Loss: 1.611\n",
      "Epoch: 2201, Loss: 1.697\n",
      "Epoch: 2201, Loss: 1.785\n",
      "Epoch: 2201, Loss: 1.900\n",
      "Epoch: 2201, Loss: 1.958\n",
      "Epoch: 2201, Loss: 2.029\n",
      "Epoch: 2201, Loss: 2.105\n",
      "Epoch: 2201, Loss: 2.339\n",
      "Epoch: 2201, Loss: 2.422\n",
      "Epoch: 2201, Loss: 2.529\n",
      "Epoch: 2201, Loss: 2.738\n",
      "Epoch: 2201, Loss: 2.853\n",
      "Epoch: 2201, Loss: 2.997\n",
      "Epoch: 2201, Loss: 3.090\n",
      "Epoch: 2201, Loss: 3.245\n",
      "Epoch: 2201, Loss: 3.339\n",
      "Epoch: 2201, Loss: 3.464\n",
      "Epoch: 2301, Loss: 0.075\n",
      "Epoch: 2301, Loss: 0.217\n",
      "Epoch: 2301, Loss: 0.371\n",
      "Epoch: 2301, Loss: 0.440\n",
      "Epoch: 2301, Loss: 0.569\n",
      "Epoch: 2301, Loss: 0.678\n",
      "Epoch: 2301, Loss: 0.731\n",
      "Epoch: 2301, Loss: 0.956\n",
      "Epoch: 2301, Loss: 1.114\n",
      "Epoch: 2301, Loss: 1.153\n",
      "Epoch: 2301, Loss: 1.202\n",
      "Epoch: 2301, Loss: 1.334\n",
      "Epoch: 2301, Loss: 1.584\n",
      "Epoch: 2301, Loss: 1.772\n",
      "Epoch: 2301, Loss: 1.838\n",
      "Epoch: 2301, Loss: 1.902\n",
      "Epoch: 2301, Loss: 2.013\n",
      "Epoch: 2301, Loss: 2.118\n",
      "Epoch: 2301, Loss: 2.226\n",
      "Epoch: 2301, Loss: 2.302\n",
      "Epoch: 2301, Loss: 2.413\n",
      "Epoch: 2301, Loss: 2.517\n",
      "Epoch: 2301, Loss: 2.623\n",
      "Epoch: 2301, Loss: 2.852\n",
      "Epoch: 2301, Loss: 3.076\n",
      "Epoch: 2301, Loss: 3.315\n",
      "Epoch: 2301, Loss: 3.395\n",
      "Epoch: 2301, Loss: 3.552\n",
      "Epoch: 2301, Loss: 3.670\n",
      "Epoch: 2301, Loss: 3.735\n",
      "Epoch: 2301, Loss: 3.847\n",
      "Epoch: 2401, Loss: 0.111\n",
      "Epoch: 2401, Loss: 0.296\n",
      "Epoch: 2401, Loss: 0.538\n",
      "Epoch: 2401, Loss: 0.740\n",
      "Epoch: 2401, Loss: 0.797\n",
      "Epoch: 2401, Loss: 0.928\n",
      "Epoch: 2401, Loss: 0.983\n",
      "Epoch: 2401, Loss: 1.069\n",
      "Epoch: 2401, Loss: 1.147\n",
      "Epoch: 2401, Loss: 1.224\n",
      "Epoch: 2401, Loss: 1.456\n",
      "Epoch: 2401, Loss: 1.556\n",
      "Epoch: 2401, Loss: 1.705\n",
      "Epoch: 2401, Loss: 1.829\n",
      "Epoch: 2401, Loss: 1.920\n",
      "Epoch: 2401, Loss: 2.001\n",
      "Epoch: 2401, Loss: 2.135\n",
      "Epoch: 2401, Loss: 2.220\n",
      "Epoch: 2401, Loss: 2.290\n",
      "Epoch: 2401, Loss: 2.338\n",
      "Epoch: 2401, Loss: 2.528\n",
      "Epoch: 2401, Loss: 2.585\n",
      "Epoch: 2401, Loss: 2.654\n",
      "Epoch: 2401, Loss: 2.726\n",
      "Epoch: 2401, Loss: 2.787\n",
      "Epoch: 2401, Loss: 2.826\n",
      "Epoch: 2401, Loss: 3.029\n",
      "Epoch: 2401, Loss: 3.119\n",
      "Epoch: 2401, Loss: 3.216\n",
      "Epoch: 2401, Loss: 3.325\n",
      "Epoch: 2401, Loss: 3.442\n",
      "Epoch: 2501, Loss: 0.107\n",
      "Epoch: 2501, Loss: 0.315\n",
      "Epoch: 2501, Loss: 0.409\n",
      "Epoch: 2501, Loss: 0.512\n",
      "Epoch: 2501, Loss: 0.680\n",
      "Epoch: 2501, Loss: 0.758\n",
      "Epoch: 2501, Loss: 0.881\n",
      "Epoch: 2501, Loss: 1.017\n",
      "Epoch: 2501, Loss: 1.219\n",
      "Epoch: 2501, Loss: 1.465\n",
      "Epoch: 2501, Loss: 1.633\n",
      "Epoch: 2501, Loss: 1.774\n",
      "Epoch: 2501, Loss: 1.837\n",
      "Epoch: 2501, Loss: 2.024\n",
      "Epoch: 2501, Loss: 2.101\n",
      "Epoch: 2501, Loss: 2.191\n",
      "Epoch: 2501, Loss: 2.357\n",
      "Epoch: 2501, Loss: 2.405\n",
      "Epoch: 2501, Loss: 2.448\n",
      "Epoch: 2501, Loss: 2.526\n",
      "Epoch: 2501, Loss: 2.724\n",
      "Epoch: 2501, Loss: 2.871\n",
      "Epoch: 2501, Loss: 2.947\n",
      "Epoch: 2501, Loss: 3.055\n",
      "Epoch: 2501, Loss: 3.186\n",
      "Epoch: 2501, Loss: 3.360\n",
      "Epoch: 2501, Loss: 3.496\n",
      "Epoch: 2501, Loss: 3.608\n",
      "Epoch: 2501, Loss: 3.926\n",
      "Epoch: 2501, Loss: 4.016\n",
      "Epoch: 2501, Loss: 4.145\n",
      "Epoch: 2601, Loss: 0.104\n",
      "Epoch: 2601, Loss: 0.220\n",
      "Epoch: 2601, Loss: 0.305\n",
      "Epoch: 2601, Loss: 0.389\n",
      "Epoch: 2601, Loss: 0.495\n",
      "Epoch: 2601, Loss: 0.613\n",
      "Epoch: 2601, Loss: 0.698\n",
      "Epoch: 2601, Loss: 0.809\n",
      "Epoch: 2601, Loss: 0.932\n",
      "Epoch: 2601, Loss: 1.049\n",
      "Epoch: 2601, Loss: 1.192\n",
      "Epoch: 2601, Loss: 1.373\n",
      "Epoch: 2601, Loss: 1.639\n",
      "Epoch: 2601, Loss: 1.841\n",
      "Epoch: 2601, Loss: 1.907\n",
      "Epoch: 2601, Loss: 2.026\n",
      "Epoch: 2601, Loss: 2.097\n",
      "Epoch: 2601, Loss: 2.223\n",
      "Epoch: 2601, Loss: 2.404\n",
      "Epoch: 2601, Loss: 2.502\n",
      "Epoch: 2601, Loss: 2.696\n",
      "Epoch: 2601, Loss: 2.760\n",
      "Epoch: 2601, Loss: 2.878\n",
      "Epoch: 2601, Loss: 2.973\n",
      "Epoch: 2601, Loss: 3.232\n",
      "Epoch: 2601, Loss: 3.284\n",
      "Epoch: 2601, Loss: 3.357\n",
      "Epoch: 2601, Loss: 3.498\n",
      "Epoch: 2601, Loss: 3.698\n",
      "Epoch: 2601, Loss: 3.735\n",
      "Epoch: 2601, Loss: 3.791\n",
      "Epoch: 2701, Loss: 0.100\n",
      "Epoch: 2701, Loss: 0.157\n",
      "Epoch: 2701, Loss: 0.208\n",
      "Epoch: 2701, Loss: 0.357\n",
      "Epoch: 2701, Loss: 0.484\n",
      "Epoch: 2701, Loss: 0.581\n",
      "Epoch: 2701, Loss: 0.642\n",
      "Epoch: 2701, Loss: 0.900\n",
      "Epoch: 2701, Loss: 0.992\n",
      "Epoch: 2701, Loss: 1.084\n",
      "Epoch: 2701, Loss: 1.171\n",
      "Epoch: 2701, Loss: 1.310\n",
      "Epoch: 2701, Loss: 1.470\n",
      "Epoch: 2701, Loss: 1.537\n",
      "Epoch: 2701, Loss: 1.603\n",
      "Epoch: 2701, Loss: 1.809\n",
      "Epoch: 2701, Loss: 1.944\n",
      "Epoch: 2701, Loss: 2.085\n",
      "Epoch: 2701, Loss: 2.221\n",
      "Epoch: 2701, Loss: 2.309\n",
      "Epoch: 2701, Loss: 2.398\n",
      "Epoch: 2701, Loss: 2.490\n",
      "Epoch: 2701, Loss: 2.549\n",
      "Epoch: 2701, Loss: 2.612\n",
      "Epoch: 2701, Loss: 2.726\n",
      "Epoch: 2701, Loss: 2.788\n",
      "Epoch: 2701, Loss: 3.001\n",
      "Epoch: 2701, Loss: 3.048\n",
      "Epoch: 2701, Loss: 3.185\n",
      "Epoch: 2701, Loss: 3.254\n",
      "Epoch: 2701, Loss: 3.496\n",
      "Epoch: 2801, Loss: 0.159\n",
      "Epoch: 2801, Loss: 0.281\n",
      "Epoch: 2801, Loss: 0.430\n",
      "Epoch: 2801, Loss: 0.606\n",
      "Epoch: 2801, Loss: 0.663\n",
      "Epoch: 2801, Loss: 0.834\n",
      "Epoch: 2801, Loss: 0.910\n",
      "Epoch: 2801, Loss: 0.985\n",
      "Epoch: 2801, Loss: 1.061\n",
      "Epoch: 2801, Loss: 1.305\n",
      "Epoch: 2801, Loss: 1.348\n",
      "Epoch: 2801, Loss: 1.441\n",
      "Epoch: 2801, Loss: 1.509\n",
      "Epoch: 2801, Loss: 1.541\n",
      "Epoch: 2801, Loss: 1.655\n",
      "Epoch: 2801, Loss: 1.776\n",
      "Epoch: 2801, Loss: 1.852\n",
      "Epoch: 2801, Loss: 1.951\n",
      "Epoch: 2801, Loss: 2.118\n",
      "Epoch: 2801, Loss: 2.325\n",
      "Epoch: 2801, Loss: 2.473\n",
      "Epoch: 2801, Loss: 2.564\n",
      "Epoch: 2801, Loss: 2.669\n",
      "Epoch: 2801, Loss: 2.773\n",
      "Epoch: 2801, Loss: 2.972\n",
      "Epoch: 2801, Loss: 3.087\n",
      "Epoch: 2801, Loss: 3.161\n",
      "Epoch: 2801, Loss: 3.380\n",
      "Epoch: 2801, Loss: 3.505\n",
      "Epoch: 2801, Loss: 3.630\n",
      "Epoch: 2801, Loss: 3.746\n",
      "Epoch: 2901, Loss: 0.163\n",
      "Epoch: 2901, Loss: 0.268\n",
      "Epoch: 2901, Loss: 0.457\n",
      "Epoch: 2901, Loss: 0.689\n",
      "Epoch: 2901, Loss: 0.742\n",
      "Epoch: 2901, Loss: 0.891\n",
      "Epoch: 2901, Loss: 1.038\n",
      "Epoch: 2901, Loss: 1.173\n",
      "Epoch: 2901, Loss: 1.308\n",
      "Epoch: 2901, Loss: 1.457\n",
      "Epoch: 2901, Loss: 1.626\n",
      "Epoch: 2901, Loss: 1.702\n",
      "Epoch: 2901, Loss: 1.732\n",
      "Epoch: 2901, Loss: 1.845\n",
      "Epoch: 2901, Loss: 1.898\n",
      "Epoch: 2901, Loss: 1.978\n",
      "Epoch: 2901, Loss: 2.060\n",
      "Epoch: 2901, Loss: 2.139\n",
      "Epoch: 2901, Loss: 2.201\n",
      "Epoch: 2901, Loss: 2.443\n",
      "Epoch: 2901, Loss: 2.552\n",
      "Epoch: 2901, Loss: 2.631\n",
      "Epoch: 2901, Loss: 2.756\n",
      "Epoch: 2901, Loss: 2.902\n",
      "Epoch: 2901, Loss: 3.068\n",
      "Epoch: 2901, Loss: 3.147\n",
      "Epoch: 2901, Loss: 3.296\n",
      "Epoch: 2901, Loss: 3.398\n",
      "Epoch: 2901, Loss: 3.573\n",
      "Epoch: 2901, Loss: 3.740\n",
      "Epoch: 2901, Loss: 3.847\n",
      "Epoch: 3001, Loss: 0.193\n",
      "Epoch: 3001, Loss: 0.409\n",
      "Epoch: 3001, Loss: 0.613\n",
      "Epoch: 3001, Loss: 0.695\n",
      "Epoch: 3001, Loss: 0.793\n",
      "Epoch: 3001, Loss: 0.921\n",
      "Epoch: 3001, Loss: 1.037\n",
      "Epoch: 3001, Loss: 1.120\n",
      "Epoch: 3001, Loss: 1.192\n",
      "Epoch: 3001, Loss: 1.300\n",
      "Epoch: 3001, Loss: 1.461\n",
      "Epoch: 3001, Loss: 1.605\n",
      "Epoch: 3001, Loss: 1.694\n",
      "Epoch: 3001, Loss: 1.760\n",
      "Epoch: 3001, Loss: 1.860\n",
      "Epoch: 3001, Loss: 1.999\n",
      "Epoch: 3001, Loss: 2.248\n",
      "Epoch: 3001, Loss: 2.455\n",
      "Epoch: 3001, Loss: 2.541\n",
      "Epoch: 3001, Loss: 2.676\n",
      "Epoch: 3001, Loss: 2.809\n",
      "Epoch: 3001, Loss: 2.988\n",
      "Epoch: 3001, Loss: 3.175\n",
      "Epoch: 3001, Loss: 3.295\n",
      "Epoch: 3001, Loss: 3.384\n",
      "Epoch: 3001, Loss: 3.463\n",
      "Epoch: 3001, Loss: 3.572\n",
      "Epoch: 3001, Loss: 3.670\n",
      "Epoch: 3001, Loss: 3.707\n",
      "Epoch: 3001, Loss: 3.781\n",
      "Epoch: 3001, Loss: 3.857\n",
      "Epoch: 3101, Loss: 0.059\n",
      "Epoch: 3101, Loss: 0.101\n",
      "Epoch: 3101, Loss: 0.146\n",
      "Epoch: 3101, Loss: 0.265\n",
      "Epoch: 3101, Loss: 0.329\n",
      "Epoch: 3101, Loss: 0.609\n",
      "Epoch: 3101, Loss: 0.726\n",
      "Epoch: 3101, Loss: 0.818\n",
      "Epoch: 3101, Loss: 0.960\n",
      "Epoch: 3101, Loss: 1.019\n",
      "Epoch: 3101, Loss: 1.104\n",
      "Epoch: 3101, Loss: 1.293\n",
      "Epoch: 3101, Loss: 1.342\n",
      "Epoch: 3101, Loss: 1.510\n",
      "Epoch: 3101, Loss: 1.663\n",
      "Epoch: 3101, Loss: 1.829\n",
      "Epoch: 3101, Loss: 1.901\n",
      "Epoch: 3101, Loss: 2.036\n",
      "Epoch: 3101, Loss: 2.180\n",
      "Epoch: 3101, Loss: 2.241\n",
      "Epoch: 3101, Loss: 2.342\n",
      "Epoch: 3101, Loss: 2.503\n",
      "Epoch: 3101, Loss: 2.674\n",
      "Epoch: 3101, Loss: 2.766\n",
      "Epoch: 3101, Loss: 2.945\n",
      "Epoch: 3101, Loss: 3.162\n",
      "Epoch: 3101, Loss: 3.212\n",
      "Epoch: 3101, Loss: 3.287\n",
      "Epoch: 3101, Loss: 3.449\n",
      "Epoch: 3101, Loss: 3.572\n",
      "Epoch: 3101, Loss: 3.635\n",
      "Epoch: 3201, Loss: 0.057\n",
      "Epoch: 3201, Loss: 0.344\n",
      "Epoch: 3201, Loss: 0.455\n",
      "Epoch: 3201, Loss: 0.628\n",
      "Epoch: 3201, Loss: 0.698\n",
      "Epoch: 3201, Loss: 0.813\n",
      "Epoch: 3201, Loss: 0.839\n",
      "Epoch: 3201, Loss: 0.958\n",
      "Epoch: 3201, Loss: 1.037\n",
      "Epoch: 3201, Loss: 1.210\n",
      "Epoch: 3201, Loss: 1.338\n",
      "Epoch: 3201, Loss: 1.432\n",
      "Epoch: 3201, Loss: 1.472\n",
      "Epoch: 3201, Loss: 1.527\n",
      "Epoch: 3201, Loss: 1.808\n",
      "Epoch: 3201, Loss: 1.999\n",
      "Epoch: 3201, Loss: 2.124\n",
      "Epoch: 3201, Loss: 2.331\n",
      "Epoch: 3201, Loss: 2.437\n",
      "Epoch: 3201, Loss: 2.555\n",
      "Epoch: 3201, Loss: 2.669\n",
      "Epoch: 3201, Loss: 2.818\n",
      "Epoch: 3201, Loss: 2.977\n",
      "Epoch: 3201, Loss: 3.116\n",
      "Epoch: 3201, Loss: 3.196\n",
      "Epoch: 3201, Loss: 3.384\n",
      "Epoch: 3201, Loss: 3.533\n",
      "Epoch: 3201, Loss: 3.603\n",
      "Epoch: 3201, Loss: 3.835\n",
      "Epoch: 3201, Loss: 3.977\n",
      "Epoch: 3201, Loss: 4.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3301, Loss: 0.095\n",
      "Epoch: 3301, Loss: 0.337\n",
      "Epoch: 3301, Loss: 0.501\n",
      "Epoch: 3301, Loss: 0.843\n",
      "Epoch: 3301, Loss: 1.039\n",
      "Epoch: 3301, Loss: 1.150\n",
      "Epoch: 3301, Loss: 1.403\n",
      "Epoch: 3301, Loss: 1.491\n",
      "Epoch: 3301, Loss: 1.565\n",
      "Epoch: 3301, Loss: 1.680\n",
      "Epoch: 3301, Loss: 1.820\n",
      "Epoch: 3301, Loss: 1.998\n",
      "Epoch: 3301, Loss: 2.117\n",
      "Epoch: 3301, Loss: 2.171\n",
      "Epoch: 3301, Loss: 2.319\n",
      "Epoch: 3301, Loss: 2.393\n",
      "Epoch: 3301, Loss: 2.579\n",
      "Epoch: 3301, Loss: 2.760\n",
      "Epoch: 3301, Loss: 2.907\n",
      "Epoch: 3301, Loss: 2.966\n",
      "Epoch: 3301, Loss: 3.087\n",
      "Epoch: 3301, Loss: 3.181\n",
      "Epoch: 3301, Loss: 3.356\n",
      "Epoch: 3301, Loss: 3.469\n",
      "Epoch: 3301, Loss: 3.603\n",
      "Epoch: 3301, Loss: 3.719\n",
      "Epoch: 3301, Loss: 3.902\n",
      "Epoch: 3301, Loss: 3.987\n",
      "Epoch: 3301, Loss: 4.088\n",
      "Epoch: 3301, Loss: 4.210\n",
      "Epoch: 3301, Loss: 4.437\n",
      "Epoch: 3401, Loss: 0.065\n",
      "Epoch: 3401, Loss: 0.146\n",
      "Epoch: 3401, Loss: 0.242\n",
      "Epoch: 3401, Loss: 0.381\n",
      "Epoch: 3401, Loss: 0.540\n",
      "Epoch: 3401, Loss: 0.617\n",
      "Epoch: 3401, Loss: 0.714\n",
      "Epoch: 3401, Loss: 0.872\n",
      "Epoch: 3401, Loss: 1.014\n",
      "Epoch: 3401, Loss: 1.096\n",
      "Epoch: 3401, Loss: 1.257\n",
      "Epoch: 3401, Loss: 1.400\n",
      "Epoch: 3401, Loss: 1.521\n",
      "Epoch: 3401, Loss: 1.640\n",
      "Epoch: 3401, Loss: 1.850\n",
      "Epoch: 3401, Loss: 1.892\n",
      "Epoch: 3401, Loss: 1.983\n",
      "Epoch: 3401, Loss: 2.049\n",
      "Epoch: 3401, Loss: 2.157\n",
      "Epoch: 3401, Loss: 2.380\n",
      "Epoch: 3401, Loss: 2.462\n",
      "Epoch: 3401, Loss: 2.670\n",
      "Epoch: 3401, Loss: 2.765\n",
      "Epoch: 3401, Loss: 2.840\n",
      "Epoch: 3401, Loss: 2.930\n",
      "Epoch: 3401, Loss: 3.070\n",
      "Epoch: 3401, Loss: 3.241\n",
      "Epoch: 3401, Loss: 3.403\n",
      "Epoch: 3401, Loss: 3.532\n",
      "Epoch: 3401, Loss: 3.718\n",
      "Epoch: 3401, Loss: 3.750\n",
      "Epoch: 3501, Loss: 0.109\n",
      "Epoch: 3501, Loss: 0.194\n",
      "Epoch: 3501, Loss: 0.306\n",
      "Epoch: 3501, Loss: 0.428\n",
      "Epoch: 3501, Loss: 0.501\n",
      "Epoch: 3501, Loss: 0.684\n",
      "Epoch: 3501, Loss: 0.791\n",
      "Epoch: 3501, Loss: 1.059\n",
      "Epoch: 3501, Loss: 1.104\n",
      "Epoch: 3501, Loss: 1.261\n",
      "Epoch: 3501, Loss: 1.484\n",
      "Epoch: 3501, Loss: 1.544\n",
      "Epoch: 3501, Loss: 1.610\n",
      "Epoch: 3501, Loss: 1.742\n",
      "Epoch: 3501, Loss: 1.934\n",
      "Epoch: 3501, Loss: 1.993\n",
      "Epoch: 3501, Loss: 2.082\n",
      "Epoch: 3501, Loss: 2.222\n",
      "Epoch: 3501, Loss: 2.341\n",
      "Epoch: 3501, Loss: 2.476\n",
      "Epoch: 3501, Loss: 2.644\n",
      "Epoch: 3501, Loss: 2.723\n",
      "Epoch: 3501, Loss: 2.780\n",
      "Epoch: 3501, Loss: 2.849\n",
      "Epoch: 3501, Loss: 2.967\n",
      "Epoch: 3501, Loss: 3.104\n",
      "Epoch: 3501, Loss: 3.314\n",
      "Epoch: 3501, Loss: 3.377\n",
      "Epoch: 3501, Loss: 3.449\n",
      "Epoch: 3501, Loss: 3.723\n",
      "Epoch: 3501, Loss: 3.916\n",
      "Epoch: 3601, Loss: 0.149\n",
      "Epoch: 3601, Loss: 0.199\n",
      "Epoch: 3601, Loss: 0.294\n",
      "Epoch: 3601, Loss: 0.434\n",
      "Epoch: 3601, Loss: 0.546\n",
      "Epoch: 3601, Loss: 0.612\n",
      "Epoch: 3601, Loss: 0.836\n",
      "Epoch: 3601, Loss: 0.927\n",
      "Epoch: 3601, Loss: 1.029\n",
      "Epoch: 3601, Loss: 1.108\n",
      "Epoch: 3601, Loss: 1.304\n",
      "Epoch: 3601, Loss: 1.373\n",
      "Epoch: 3601, Loss: 1.488\n",
      "Epoch: 3601, Loss: 1.566\n",
      "Epoch: 3601, Loss: 1.677\n",
      "Epoch: 3601, Loss: 1.795\n",
      "Epoch: 3601, Loss: 1.907\n",
      "Epoch: 3601, Loss: 2.131\n",
      "Epoch: 3601, Loss: 2.195\n",
      "Epoch: 3601, Loss: 2.275\n",
      "Epoch: 3601, Loss: 2.394\n",
      "Epoch: 3601, Loss: 2.493\n",
      "Epoch: 3601, Loss: 2.560\n",
      "Epoch: 3601, Loss: 2.691\n",
      "Epoch: 3601, Loss: 2.794\n",
      "Epoch: 3601, Loss: 2.974\n",
      "Epoch: 3601, Loss: 3.082\n",
      "Epoch: 3601, Loss: 3.245\n",
      "Epoch: 3601, Loss: 3.363\n",
      "Epoch: 3601, Loss: 3.490\n",
      "Epoch: 3601, Loss: 3.569\n",
      "Epoch: 3701, Loss: 0.097\n",
      "Epoch: 3701, Loss: 0.215\n",
      "Epoch: 3701, Loss: 0.304\n",
      "Epoch: 3701, Loss: 0.538\n",
      "Epoch: 3701, Loss: 0.664\n",
      "Epoch: 3701, Loss: 0.795\n",
      "Epoch: 3701, Loss: 0.922\n",
      "Epoch: 3701, Loss: 0.947\n",
      "Epoch: 3701, Loss: 1.047\n",
      "Epoch: 3701, Loss: 1.171\n",
      "Epoch: 3701, Loss: 1.285\n",
      "Epoch: 3701, Loss: 1.447\n",
      "Epoch: 3701, Loss: 1.590\n",
      "Epoch: 3701, Loss: 1.707\n",
      "Epoch: 3701, Loss: 1.801\n",
      "Epoch: 3701, Loss: 1.914\n",
      "Epoch: 3701, Loss: 2.123\n",
      "Epoch: 3701, Loss: 2.233\n",
      "Epoch: 3701, Loss: 2.299\n",
      "Epoch: 3701, Loss: 2.388\n",
      "Epoch: 3701, Loss: 2.434\n",
      "Epoch: 3701, Loss: 2.561\n",
      "Epoch: 3701, Loss: 2.756\n",
      "Epoch: 3701, Loss: 2.901\n",
      "Epoch: 3701, Loss: 2.994\n",
      "Epoch: 3701, Loss: 3.151\n",
      "Epoch: 3701, Loss: 3.311\n",
      "Epoch: 3701, Loss: 3.443\n",
      "Epoch: 3701, Loss: 3.525\n",
      "Epoch: 3701, Loss: 3.585\n",
      "Epoch: 3701, Loss: 3.695\n",
      "Epoch: 3801, Loss: 0.094\n",
      "Epoch: 3801, Loss: 0.216\n",
      "Epoch: 3801, Loss: 0.307\n",
      "Epoch: 3801, Loss: 0.402\n",
      "Epoch: 3801, Loss: 0.652\n",
      "Epoch: 3801, Loss: 0.790\n",
      "Epoch: 3801, Loss: 0.888\n",
      "Epoch: 3801, Loss: 1.002\n",
      "Epoch: 3801, Loss: 1.157\n",
      "Epoch: 3801, Loss: 1.265\n",
      "Epoch: 3801, Loss: 1.358\n",
      "Epoch: 3801, Loss: 1.448\n",
      "Epoch: 3801, Loss: 1.541\n",
      "Epoch: 3801, Loss: 1.697\n",
      "Epoch: 3801, Loss: 1.809\n",
      "Epoch: 3801, Loss: 1.870\n",
      "Epoch: 3801, Loss: 1.961\n",
      "Epoch: 3801, Loss: 2.094\n",
      "Epoch: 3801, Loss: 2.213\n",
      "Epoch: 3801, Loss: 2.291\n",
      "Epoch: 3801, Loss: 2.364\n",
      "Epoch: 3801, Loss: 2.561\n",
      "Epoch: 3801, Loss: 2.652\n",
      "Epoch: 3801, Loss: 2.719\n",
      "Epoch: 3801, Loss: 2.824\n",
      "Epoch: 3801, Loss: 2.942\n",
      "Epoch: 3801, Loss: 3.121\n",
      "Epoch: 3801, Loss: 3.175\n",
      "Epoch: 3801, Loss: 3.294\n",
      "Epoch: 3801, Loss: 3.378\n",
      "Epoch: 3801, Loss: 3.525\n",
      "Epoch: 3901, Loss: 0.129\n",
      "Epoch: 3901, Loss: 0.206\n",
      "Epoch: 3901, Loss: 0.360\n",
      "Epoch: 3901, Loss: 0.424\n",
      "Epoch: 3901, Loss: 0.546\n",
      "Epoch: 3901, Loss: 0.604\n",
      "Epoch: 3901, Loss: 0.703\n",
      "Epoch: 3901, Loss: 0.894\n",
      "Epoch: 3901, Loss: 0.971\n",
      "Epoch: 3901, Loss: 1.066\n",
      "Epoch: 3901, Loss: 1.252\n",
      "Epoch: 3901, Loss: 1.317\n",
      "Epoch: 3901, Loss: 1.489\n",
      "Epoch: 3901, Loss: 1.585\n",
      "Epoch: 3901, Loss: 1.688\n",
      "Epoch: 3901, Loss: 1.853\n",
      "Epoch: 3901, Loss: 2.097\n",
      "Epoch: 3901, Loss: 2.271\n",
      "Epoch: 3901, Loss: 2.381\n",
      "Epoch: 3901, Loss: 2.594\n",
      "Epoch: 3901, Loss: 2.633\n",
      "Epoch: 3901, Loss: 2.772\n",
      "Epoch: 3901, Loss: 2.855\n",
      "Epoch: 3901, Loss: 3.175\n",
      "Epoch: 3901, Loss: 3.266\n",
      "Epoch: 3901, Loss: 3.397\n",
      "Epoch: 3901, Loss: 3.509\n",
      "Epoch: 3901, Loss: 3.580\n",
      "Epoch: 3901, Loss: 3.724\n",
      "Epoch: 3901, Loss: 3.768\n",
      "Epoch: 3901, Loss: 3.973\n",
      "Epoch: 4001, Loss: 0.122\n",
      "Epoch: 4001, Loss: 0.232\n",
      "Epoch: 4001, Loss: 0.302\n",
      "Epoch: 4001, Loss: 0.429\n",
      "Epoch: 4001, Loss: 0.618\n",
      "Epoch: 4001, Loss: 0.750\n",
      "Epoch: 4001, Loss: 0.884\n",
      "Epoch: 4001, Loss: 0.948\n",
      "Epoch: 4001, Loss: 1.212\n",
      "Epoch: 4001, Loss: 1.325\n",
      "Epoch: 4001, Loss: 1.422\n",
      "Epoch: 4001, Loss: 1.520\n",
      "Epoch: 4001, Loss: 1.578\n",
      "Epoch: 4001, Loss: 1.685\n",
      "Epoch: 4001, Loss: 1.881\n",
      "Epoch: 4001, Loss: 1.960\n",
      "Epoch: 4001, Loss: 2.068\n",
      "Epoch: 4001, Loss: 2.330\n",
      "Epoch: 4001, Loss: 2.523\n",
      "Epoch: 4001, Loss: 2.616\n",
      "Epoch: 4001, Loss: 2.844\n",
      "Epoch: 4001, Loss: 3.082\n",
      "Epoch: 4001, Loss: 3.207\n",
      "Epoch: 4001, Loss: 3.384\n",
      "Epoch: 4001, Loss: 3.472\n",
      "Epoch: 4001, Loss: 3.690\n",
      "Epoch: 4001, Loss: 3.873\n",
      "Epoch: 4001, Loss: 4.110\n",
      "Epoch: 4001, Loss: 4.203\n",
      "Epoch: 4001, Loss: 4.271\n",
      "Epoch: 4001, Loss: 4.461\n",
      "Epoch: 4101, Loss: 0.089\n",
      "Epoch: 4101, Loss: 0.265\n",
      "Epoch: 4101, Loss: 0.344\n",
      "Epoch: 4101, Loss: 0.784\n",
      "Epoch: 4101, Loss: 0.869\n",
      "Epoch: 4101, Loss: 1.114\n",
      "Epoch: 4101, Loss: 1.172\n",
      "Epoch: 4101, Loss: 1.287\n",
      "Epoch: 4101, Loss: 1.420\n",
      "Epoch: 4101, Loss: 1.506\n",
      "Epoch: 4101, Loss: 1.607\n",
      "Epoch: 4101, Loss: 1.744\n",
      "Epoch: 4101, Loss: 1.832\n",
      "Epoch: 4101, Loss: 1.893\n",
      "Epoch: 4101, Loss: 2.064\n",
      "Epoch: 4101, Loss: 2.183\n",
      "Epoch: 4101, Loss: 2.265\n",
      "Epoch: 4101, Loss: 2.332\n",
      "Epoch: 4101, Loss: 2.546\n",
      "Epoch: 4101, Loss: 2.829\n",
      "Epoch: 4101, Loss: 2.948\n",
      "Epoch: 4101, Loss: 3.035\n",
      "Epoch: 4101, Loss: 3.427\n",
      "Epoch: 4101, Loss: 3.482\n",
      "Epoch: 4101, Loss: 3.587\n",
      "Epoch: 4101, Loss: 3.649\n",
      "Epoch: 4101, Loss: 3.760\n",
      "Epoch: 4101, Loss: 3.857\n",
      "Epoch: 4101, Loss: 3.940\n",
      "Epoch: 4101, Loss: 4.016\n",
      "Epoch: 4101, Loss: 4.100\n",
      "Epoch: 4201, Loss: 0.076\n",
      "Epoch: 4201, Loss: 0.234\n",
      "Epoch: 4201, Loss: 0.350\n",
      "Epoch: 4201, Loss: 0.447\n",
      "Epoch: 4201, Loss: 0.545\n",
      "Epoch: 4201, Loss: 0.623\n",
      "Epoch: 4201, Loss: 0.831\n",
      "Epoch: 4201, Loss: 0.934\n",
      "Epoch: 4201, Loss: 1.016\n",
      "Epoch: 4201, Loss: 1.200\n",
      "Epoch: 4201, Loss: 1.320\n",
      "Epoch: 4201, Loss: 1.398\n",
      "Epoch: 4201, Loss: 1.537\n",
      "Epoch: 4201, Loss: 1.694\n",
      "Epoch: 4201, Loss: 1.829\n",
      "Epoch: 4201, Loss: 1.879\n",
      "Epoch: 4201, Loss: 2.007\n",
      "Epoch: 4201, Loss: 2.106\n",
      "Epoch: 4201, Loss: 2.241\n",
      "Epoch: 4201, Loss: 2.413\n",
      "Epoch: 4201, Loss: 2.528\n",
      "Epoch: 4201, Loss: 2.649\n",
      "Epoch: 4201, Loss: 2.767\n",
      "Epoch: 4201, Loss: 2.806\n",
      "Epoch: 4201, Loss: 2.870\n",
      "Epoch: 4201, Loss: 3.005\n",
      "Epoch: 4201, Loss: 3.134\n",
      "Epoch: 4201, Loss: 3.289\n",
      "Epoch: 4201, Loss: 3.376\n",
      "Epoch: 4201, Loss: 3.543\n",
      "Epoch: 4201, Loss: 3.599\n",
      "Epoch: 4301, Loss: 0.162\n",
      "Epoch: 4301, Loss: 0.263\n",
      "Epoch: 4301, Loss: 0.327\n",
      "Epoch: 4301, Loss: 0.499\n",
      "Epoch: 4301, Loss: 0.599\n",
      "Epoch: 4301, Loss: 0.736\n",
      "Epoch: 4301, Loss: 0.828\n",
      "Epoch: 4301, Loss: 0.954\n",
      "Epoch: 4301, Loss: 1.091\n",
      "Epoch: 4301, Loss: 1.302\n",
      "Epoch: 4301, Loss: 1.346\n",
      "Epoch: 4301, Loss: 1.529\n",
      "Epoch: 4301, Loss: 1.616\n",
      "Epoch: 4301, Loss: 1.709\n",
      "Epoch: 4301, Loss: 2.008\n",
      "Epoch: 4301, Loss: 2.102\n",
      "Epoch: 4301, Loss: 2.220\n",
      "Epoch: 4301, Loss: 2.290\n",
      "Epoch: 4301, Loss: 2.362\n",
      "Epoch: 4301, Loss: 2.469\n",
      "Epoch: 4301, Loss: 2.621\n",
      "Epoch: 4301, Loss: 2.704\n",
      "Epoch: 4301, Loss: 2.938\n",
      "Epoch: 4301, Loss: 3.020\n",
      "Epoch: 4301, Loss: 3.180\n",
      "Epoch: 4301, Loss: 3.395\n",
      "Epoch: 4301, Loss: 3.507\n",
      "Epoch: 4301, Loss: 3.782\n",
      "Epoch: 4301, Loss: 3.880\n",
      "Epoch: 4301, Loss: 3.934\n",
      "Epoch: 4301, Loss: 4.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4401, Loss: 0.087\n",
      "Epoch: 4401, Loss: 0.186\n",
      "Epoch: 4401, Loss: 0.243\n",
      "Epoch: 4401, Loss: 0.334\n",
      "Epoch: 4401, Loss: 0.415\n",
      "Epoch: 4401, Loss: 0.490\n",
      "Epoch: 4401, Loss: 0.559\n",
      "Epoch: 4401, Loss: 0.709\n",
      "Epoch: 4401, Loss: 0.959\n",
      "Epoch: 4401, Loss: 1.062\n",
      "Epoch: 4401, Loss: 1.152\n",
      "Epoch: 4401, Loss: 1.238\n",
      "Epoch: 4401, Loss: 1.298\n",
      "Epoch: 4401, Loss: 1.443\n",
      "Epoch: 4401, Loss: 1.567\n",
      "Epoch: 4401, Loss: 1.683\n",
      "Epoch: 4401, Loss: 1.783\n",
      "Epoch: 4401, Loss: 1.940\n",
      "Epoch: 4401, Loss: 2.185\n",
      "Epoch: 4401, Loss: 2.278\n",
      "Epoch: 4401, Loss: 2.622\n",
      "Epoch: 4401, Loss: 2.769\n",
      "Epoch: 4401, Loss: 2.941\n",
      "Epoch: 4401, Loss: 3.118\n",
      "Epoch: 4401, Loss: 3.320\n",
      "Epoch: 4401, Loss: 3.398\n",
      "Epoch: 4401, Loss: 3.445\n",
      "Epoch: 4401, Loss: 3.605\n",
      "Epoch: 4401, Loss: 3.840\n",
      "Epoch: 4401, Loss: 3.984\n",
      "Epoch: 4401, Loss: 4.049\n",
      "Epoch: 4501, Loss: 0.069\n",
      "Epoch: 4501, Loss: 0.135\n",
      "Epoch: 4501, Loss: 0.227\n",
      "Epoch: 4501, Loss: 0.345\n",
      "Epoch: 4501, Loss: 0.441\n",
      "Epoch: 4501, Loss: 0.501\n",
      "Epoch: 4501, Loss: 0.669\n",
      "Epoch: 4501, Loss: 0.771\n",
      "Epoch: 4501, Loss: 0.975\n",
      "Epoch: 4501, Loss: 1.107\n",
      "Epoch: 4501, Loss: 1.218\n",
      "Epoch: 4501, Loss: 1.283\n",
      "Epoch: 4501, Loss: 1.435\n",
      "Epoch: 4501, Loss: 1.583\n",
      "Epoch: 4501, Loss: 1.675\n",
      "Epoch: 4501, Loss: 1.755\n",
      "Epoch: 4501, Loss: 1.829\n",
      "Epoch: 4501, Loss: 1.911\n",
      "Epoch: 4501, Loss: 1.954\n",
      "Epoch: 4501, Loss: 2.074\n",
      "Epoch: 4501, Loss: 2.258\n",
      "Epoch: 4501, Loss: 2.468\n",
      "Epoch: 4501, Loss: 2.574\n",
      "Epoch: 4501, Loss: 2.702\n",
      "Epoch: 4501, Loss: 2.772\n",
      "Epoch: 4501, Loss: 2.846\n",
      "Epoch: 4501, Loss: 2.932\n",
      "Epoch: 4501, Loss: 3.044\n",
      "Epoch: 4501, Loss: 3.211\n",
      "Epoch: 4501, Loss: 3.319\n",
      "Epoch: 4501, Loss: 3.444\n",
      "Epoch: 4601, Loss: 0.135\n",
      "Epoch: 4601, Loss: 0.260\n",
      "Epoch: 4601, Loss: 0.315\n",
      "Epoch: 4601, Loss: 0.435\n",
      "Epoch: 4601, Loss: 0.527\n",
      "Epoch: 4601, Loss: 0.662\n",
      "Epoch: 4601, Loss: 0.699\n",
      "Epoch: 4601, Loss: 0.779\n",
      "Epoch: 4601, Loss: 0.875\n",
      "Epoch: 4601, Loss: 1.021\n",
      "Epoch: 4601, Loss: 1.108\n",
      "Epoch: 4601, Loss: 1.380\n",
      "Epoch: 4601, Loss: 1.453\n",
      "Epoch: 4601, Loss: 1.552\n",
      "Epoch: 4601, Loss: 1.619\n",
      "Epoch: 4601, Loss: 1.683\n",
      "Epoch: 4601, Loss: 1.810\n",
      "Epoch: 4601, Loss: 2.004\n",
      "Epoch: 4601, Loss: 2.117\n",
      "Epoch: 4601, Loss: 2.230\n",
      "Epoch: 4601, Loss: 2.417\n",
      "Epoch: 4601, Loss: 2.629\n",
      "Epoch: 4601, Loss: 2.689\n",
      "Epoch: 4601, Loss: 3.038\n",
      "Epoch: 4601, Loss: 3.209\n",
      "Epoch: 4601, Loss: 3.275\n",
      "Epoch: 4601, Loss: 3.580\n",
      "Epoch: 4601, Loss: 3.617\n",
      "Epoch: 4601, Loss: 3.738\n",
      "Epoch: 4601, Loss: 3.802\n",
      "Epoch: 4601, Loss: 3.940\n",
      "Epoch: 4701, Loss: 0.104\n",
      "Epoch: 4701, Loss: 0.145\n",
      "Epoch: 4701, Loss: 0.234\n",
      "Epoch: 4701, Loss: 0.443\n",
      "Epoch: 4701, Loss: 0.606\n",
      "Epoch: 4701, Loss: 0.747\n",
      "Epoch: 4701, Loss: 0.809\n",
      "Epoch: 4701, Loss: 0.915\n",
      "Epoch: 4701, Loss: 1.134\n",
      "Epoch: 4701, Loss: 1.180\n",
      "Epoch: 4701, Loss: 1.249\n",
      "Epoch: 4701, Loss: 1.335\n",
      "Epoch: 4701, Loss: 1.400\n",
      "Epoch: 4701, Loss: 1.543\n",
      "Epoch: 4701, Loss: 1.637\n",
      "Epoch: 4701, Loss: 1.719\n",
      "Epoch: 4701, Loss: 1.887\n",
      "Epoch: 4701, Loss: 2.046\n",
      "Epoch: 4701, Loss: 2.118\n",
      "Epoch: 4701, Loss: 2.174\n",
      "Epoch: 4701, Loss: 2.318\n",
      "Epoch: 4701, Loss: 2.380\n",
      "Epoch: 4701, Loss: 2.534\n",
      "Epoch: 4701, Loss: 2.619\n",
      "Epoch: 4701, Loss: 2.781\n",
      "Epoch: 4701, Loss: 2.939\n",
      "Epoch: 4701, Loss: 3.023\n",
      "Epoch: 4701, Loss: 3.079\n",
      "Epoch: 4701, Loss: 3.173\n",
      "Epoch: 4701, Loss: 3.447\n",
      "Epoch: 4701, Loss: 3.533\n",
      "Epoch: 4801, Loss: 0.132\n",
      "Epoch: 4801, Loss: 0.295\n",
      "Epoch: 4801, Loss: 0.355\n",
      "Epoch: 4801, Loss: 0.518\n",
      "Epoch: 4801, Loss: 0.674\n",
      "Epoch: 4801, Loss: 0.957\n",
      "Epoch: 4801, Loss: 1.088\n",
      "Epoch: 4801, Loss: 1.276\n",
      "Epoch: 4801, Loss: 1.353\n",
      "Epoch: 4801, Loss: 1.549\n",
      "Epoch: 4801, Loss: 1.645\n",
      "Epoch: 4801, Loss: 1.776\n",
      "Epoch: 4801, Loss: 1.896\n",
      "Epoch: 4801, Loss: 1.962\n",
      "Epoch: 4801, Loss: 2.168\n",
      "Epoch: 4801, Loss: 2.336\n",
      "Epoch: 4801, Loss: 2.395\n",
      "Epoch: 4801, Loss: 2.410\n",
      "Epoch: 4801, Loss: 2.534\n",
      "Epoch: 4801, Loss: 2.675\n",
      "Epoch: 4801, Loss: 2.759\n",
      "Epoch: 4801, Loss: 2.803\n",
      "Epoch: 4801, Loss: 2.953\n",
      "Epoch: 4801, Loss: 3.140\n",
      "Epoch: 4801, Loss: 3.322\n",
      "Epoch: 4801, Loss: 3.357\n",
      "Epoch: 4801, Loss: 3.412\n",
      "Epoch: 4801, Loss: 3.470\n",
      "Epoch: 4801, Loss: 3.559\n",
      "Epoch: 4801, Loss: 3.630\n",
      "Epoch: 4801, Loss: 3.826\n",
      "Epoch: 4901, Loss: 0.088\n",
      "Epoch: 4901, Loss: 0.192\n",
      "Epoch: 4901, Loss: 0.313\n",
      "Epoch: 4901, Loss: 0.440\n",
      "Epoch: 4901, Loss: 0.561\n",
      "Epoch: 4901, Loss: 0.709\n",
      "Epoch: 4901, Loss: 0.809\n",
      "Epoch: 4901, Loss: 0.954\n",
      "Epoch: 4901, Loss: 1.167\n",
      "Epoch: 4901, Loss: 1.317\n",
      "Epoch: 4901, Loss: 1.378\n",
      "Epoch: 4901, Loss: 1.442\n",
      "Epoch: 4901, Loss: 1.505\n",
      "Epoch: 4901, Loss: 1.585\n",
      "Epoch: 4901, Loss: 1.688\n",
      "Epoch: 4901, Loss: 1.789\n",
      "Epoch: 4901, Loss: 1.858\n",
      "Epoch: 4901, Loss: 1.954\n",
      "Epoch: 4901, Loss: 2.104\n",
      "Epoch: 4901, Loss: 2.165\n",
      "Epoch: 4901, Loss: 2.371\n",
      "Epoch: 4901, Loss: 2.464\n",
      "Epoch: 4901, Loss: 2.628\n",
      "Epoch: 4901, Loss: 2.706\n",
      "Epoch: 4901, Loss: 2.948\n",
      "Epoch: 4901, Loss: 3.033\n",
      "Epoch: 4901, Loss: 3.261\n",
      "Epoch: 4901, Loss: 3.375\n",
      "Epoch: 4901, Loss: 3.463\n",
      "Epoch: 4901, Loss: 3.536\n",
      "Epoch: 4901, Loss: 3.774\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size etc!\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "#Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "#Normalize data\n",
    "X_ = (X_ - np.mean(X_,axis=0))/np.std(X_,axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden,1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "#Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        #Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples = batch_size)\n",
    "        \n",
    "        # Reset value of X and y Input\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "        \n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_,graph)\n",
    "        \n",
    "        # Step 3\n",
    "        rate =1e-2\n",
    "        \n",
    "        sgd_update(trainables, rate)\n",
    "        \n",
    "        loss += graph[-1].value\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('Epoch: {}, Loss: {:.3f}'.format(i+1, loss/steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
