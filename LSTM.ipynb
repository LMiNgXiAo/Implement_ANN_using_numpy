{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x/e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](lstm_cell_forward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell\n",
    "    \n",
    "    xt: Input data at timestep 't', shape=(n_x,m)\n",
    "    a_prev: Hidden state at timestep 't-1', shape=(n_a,m)\n",
    "    c_prev: Memory state at timestep 't-1', shape=(n_a,m)\n",
    "    \n",
    "    Returns:\n",
    "       a_next: next hidden state. shape=(n_a,m)\n",
    "       c_next: next memory state. shape=(n_a,m)\n",
    "       yt_pred: prediction at timestep't'. shape=(n_y,m)\n",
    "       cache: tuple of values needed for the backward pass,\n",
    "              contins(a_next, c_next,a_prev,c_prev,xt,parameters)\n",
    "    \"\"\"\n",
    "    #Wf: Weight matrix of th forget gate. shape=(n_a,n_a+n_x)\n",
    "    #bf: Bias of forget gate. shape=(n_a,1)\n",
    "    #Wi: Weight matrix of the update gate. shape=(n_a,n_a+n_x)\n",
    "    #bi: Bias of update gate. shape=(n_a,1)\n",
    "    #Wc: Weight matrix of the first \"tanh\". shape=(n_a,n_a+n_x)\n",
    "    #bc: Bias of the first 'tanh'. shape=(n_a,1)\n",
    "    #Wy: Weight matrix relating the hidden state to the output. shape=(n_y,n_a)\n",
    "    #by: Bias relating the hidden state to the output. shape=(n_y,1)\n",
    "    \n",
    "    #Retrieve parameters from 'parameters'\n",
    "    Wf = parameters['Wf']\n",
    "    bf = parameters['bf']\n",
    "    Wi = parameters['Wi']\n",
    "    bi = parameters['bi']\n",
    "    Wc = parameters['Wc']\n",
    "    bc = parameters['bc']\n",
    "    Wo = parameters['Wo']\n",
    "    bo = parameters['bo']\n",
    "    Wy = parameters['Wy']\n",
    "    by = parameters['by']\n",
    "    \n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    concat = np.concatenate((a_prev,xt),axis=0)\n",
    "    \n",
    "    ft = sigmoid(np.dot(Wf,concat)+bf)\n",
    "    it = sigmoid(np.dot(Wi,concat)+bi)\n",
    "    cct = np.tanh(np.dot(Wc,concat)+bc)\n",
    "    c_next = ft*c_prev+it*cct\n",
    "    ot = sigmoid(np.dot(Wo,concat)+bo)\n",
    "    a_next = ot*np.tanh(c_next)\n",
    "    \n",
    "    yt_pred = softmax(np.dot(Wy,a_next)+by)\n",
    "    \n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt,parameters)\n",
    "    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](lstm_forward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the RNN using lstm cell.\n",
    "    \n",
    "    x: Input data for every time-step. shape=(n_x,m,T_x)\n",
    "    a0: Initial hidden state. shape=(n_a,m)\n",
    "    \n",
    "    Returns:\n",
    "    a: Hidden states for every time-step. shape=(n_a,m,T_x)\n",
    "    y: Predictions for every time-step. shape=(n_y,m,T_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    \n",
    "    n_x, m, T_x =  x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape\n",
    "    \n",
    "    # initialize 'a','c' and 'y'\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    a_next = a0\n",
    "    c_next = np.zeros((n_a,m))\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)\n",
    "        a[:,:,t] = a_next\n",
    "        y[:,:,t] = yt\n",
    "        c[:,:,t] = c_next\n",
    "        caches.append(cache)\n",
    "    \n",
    "    caches = (caches,x)\n",
    "    return a,y,c,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.17211776753291672\n",
      "a.shape =  (5, 10, 7)\n",
      "y[1][4][3] = 0.9508734618501101\n",
      "y.shape =  (2, 10, 7)\n",
      "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
      "  0.41005165]\n",
      "c[1][2][1] -0.8555449167181982\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for the LSTM-cell\n",
    "    \n",
    "    da_next: Gradients of next hidden state. shape=(n_a,m)\n",
    "    dc_next: Gradients of next cell state. shape=(n_a,m)\n",
    "    cache: cache storing information from the forward pass\n",
    "    \n",
    "    Returns:\n",
    "            dxt: Gradient of input data at time-step t. shape=(n_x,m)\n",
    "            da_prev: Gradient of the previous hidden state. shape=(n_x,m)\n",
    "            dc_prev: Gradient of the previous memory state. shape=(n_a,m,T_x)\n",
    "            dWf: Gradient of the weight matrix of the forget gate. shape=(n_a,n_a+n_x)\n",
    "            dWi: Gradient of the weight matrix of the update gate. shape=(n_a,n_a+n_x)\n",
    "            dWc: Gradient of the weight matrix of the memory gate. shape=(n_a,n_a+n_x)\n",
    "            dWo: Gradient of the weight matrix of the output gate. shape=(n_a,n_a+n_x)\n",
    "            dbf: Gradient of biases of the forget gate. shape=(n_a,1)\n",
    "            dbi: Gradient of biases of the update gate. shape=(n_a,1)\n",
    "            dbc: Gradient of biases of the memory gate. shape=(n_a,1)\n",
    "            dbo: Gradient of biases of the output gate. shape=(n_a,1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Retrieve information\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)=cache\n",
    "    \n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_next.shape\n",
    "    \n",
    "    dot = da_next*np.tanh(c_next)*ot*(1-ot)\n",
    "    dcct = (dc_next*it+ot*(1-np.square(np.tanh(c_next)))*it*da_next)*(1-np.square(cct))\n",
    "    dit = (dc_next*cct+ot*(1-np.square(np.tanh(c_next)))*cct*da_next)*it*(1-it)\n",
    "    dft = (dc_next*c_prev+ot*(1-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(1-ft)\n",
    "    \n",
    "    dWf = np.dot(dft,np.concatenate((a_prev,xt),axis=0).T)\n",
    "    dWi = np.dot(dit,np.concatenate((a_prev,xt),axis=0).T)\n",
    "    dWc = np.dot(dcct,np.concatenate((a_prev,xt),axis=0).T)\n",
    "    dWo = np.dot(dot,np.concatenate((a_prev,xt),axis=0).T)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)\n",
    "    dbi = np.sum(dit,axis=1,keepdims=True)\n",
    "    dbc = np.sum(dcct,axis=1,keepdims=True)\n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)\n",
    "    \n",
    "    da_prev = np.dot(parameters['Wf'][:,:n_a].T,dft)+np.dot(parameters['Wi'][:,:n_a].T,dit) \\\n",
    "             +np.dot(parameters['Wc'][:,:n_a].T,dcct)+np.dot(parameters['Wo'][:,:n_a].T,dot)\n",
    "    dc_prev = dc_next*ft+ot*(1-np.square(np.tanh(c_next)))*ft*da_next\n",
    "    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wi'][:,n_a:].T,dit) \\\n",
    "            +np.dot(parameters['Wc'][:,n_a:].T,dcct)+np.dot(parameters['Wo'][:,n_a:].T,dot)\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the backward pass for the RNN with LSTM-cell.\n",
    "\n",
    "    da: Gradients w.r.t the hidden states. shape (n_a, m, T_x)\n",
    "    dc: Gradients w.r.t the memory states. shape (n_a, m, T_x)\n",
    "    caches -- cache storing information from the forward pass (lstm_forward)\n",
    "\n",
    "    Returns:\n",
    "          dx: Gradient of inputs.(n_x, m, T_x)\n",
    "          da0: Gradient w.r.t. the previous hidden state. shape (n_a, m)\n",
    "          dWf: Gradient w.r.t. the weight matrix of the forget gate. shape (n_a, n_a + n_x)\n",
    "          dWi: Gradient w.r.t. the weight matrix of the update gate. shape (n_a, n_a + n_x)\n",
    "          dWc: Gradient w.r.t. the weight matrix of the memory gate. shape (n_a, n_a + n_x)\n",
    "          dWo: Gradient w.r.t. the weight matrix of the save gate. shape (n_a, n_a + n_x)\n",
    "          dbf: Gradient w.r.t. biases of the forget gate. shape (n_a, 1)\n",
    "          dbi: Gradient w.r.t. biases of the update gate. shape (n_a, 1)\n",
    "          dbc: Gradient w.r.t. biases of the memory gate. shape (n_a, 1)\n",
    "          dbo: Gradient w.r.t. biases of the save gate. shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from the first cache (t=1) of caches.\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # initialize the gradients with the right sizes (≈12 lines)\n",
    "    dx = np.zeros((n_x,m,T_x))\n",
    "    da0 = np.zeros((n_a,m))\n",
    "    da_prevt = np.zeros((n_a,m))\n",
    "    dc_prevt = np.zeros((n_a,m))\n",
    "    dWf = np.zeros((n_a,n_a+n_x))\n",
    "    dWi = np.zeros((n_a,n_a+n_x))\n",
    "    dWc = np.zeros((n_a,n_a+n_x))\n",
    "    dWo = np.zeros((n_a,n_a+n_x))\n",
    "    dbf = np.zeros((n_a,1))\n",
    "    dbi = np.zeros((n_a,1))\n",
    "    dbc = np.zeros((n_a,1))\n",
    "    dbo = np.zeros((n_a,1))\n",
    "    \n",
    "    # loop back over the whole sequence\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute all gradients using lstm_cell_backward\n",
    "        gradients = lstm_cell_backward(da[:,:,t]+da_prevt,dc_prevt,caches[t])\n",
    "        # Store or add the gradient to the parameters' previous step's gradient\n",
    "        dx[:,:,t] = gradients['dxt']\n",
    "        dWf = gradients['dWf']\n",
    "        dWi = gradients['dWi']\n",
    "        dWc = gradients['dWc']\n",
    "        dWo = gradients['dWo']\n",
    "        dbf = gradients['dbf']\n",
    "        dbi = gradients['dbi']\n",
    "        dbc = gradients['dbc']\n",
    "        dbo = gradients['dbo']\n",
    "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
    "    da0 = gradients['da_prev']\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
